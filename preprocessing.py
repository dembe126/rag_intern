import os                                                           # Standard Python-Modul f√ºr Betriebssystemfunktionen wie Dateipfad-Operationen
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importiert den TextSplitter, um den Text bzw. Dokumente in kleinere Abschnitte (Chunks) zu unterteilen
from langchain.schema import Document                                # Importiert die Document-Klasse, um Dokumente zu erstellen und zu verwalten
from langchain_community.vectorstores import Chroma                 # Importiert die Chroma-Klasse, um Vektorspeicher zu erstellen und zu verwalten
from langchain_huggingface import HuggingFaceEmbeddings               # Importiert die HuggingFaceEmbeddings-Klasse, um Text in Vektoren umzuwandeln
from config import EMBEDDING_MODEL_NAME, DB_BASE_PATH, DB_NAME               # Importiert Konfigurationen
from docling.document_converter import DocumentConverter            # Neuer docling Import


def load_document(file_path):
    '''
    Diese Funktion l√§dt eine PDF-Datei mit Docling und gibt den Inhalt als Liste von Dokumenten zur√ºck.
    '''

    if not os.path.exists(file_path):                       # √úberpr√ºft, ob die Datei existiert
        print(f"Datei nicht gefunden: {file_path}")         # Wenn nicht, wird eine Fehlermeldung ausgegeben
        return []                                           # und eine leere Liste zur√ºckgegeben
    
    converter = DocumentConverter()                         # Docling Converter erstellen

    result = converter.convert(file_path)                   # PDF mit Docling konvertieren
    docling_doc = result.document
        
    doc_name = os.path.splitext(os.path.basename(file_path))[0]     # Dokumentname ohne Pfad und Endung extrahieren
    
    full_text = docling_doc.export_to_markdown()            # Text aus Docling-Dokument extrahieren (als Markdown)

    # LangChain Document erstellen (√§hnlich wie PyMuPDF, aber nur ein Document statt eins pro Seite)
    document = [Document(
        page_content=full_text,
        metadata={
            'document_name': doc_name,
            'source': file_path,
            'extraction_method': 'docling',
            'total_pages': len(docling_doc.pages) if hasattr(docling_doc, 'pages') else 1
        }
    )]

    print(f"üìÑ Das Dokument wurde erfolgreich mit Docling geladen")
    print(f"üìè Extrahierte Textl√§nge: {len(full_text)} Zeichen")
    if hasattr(docling_doc, 'pages'):
        print(f"üìë Originalseiten: {len(docling_doc.pages)}")
    return document                                         # Gibt die Liste der Dokumente (Seiten) zur√ºck.     


def load_all_pdfs_in_folder():
    '''
    L√§dt alle PDF-Dateien aus dem aktuellen Ordner.
    '''
    pdf_files = [f for f in os.listdir(".") if f.lower().endswith(".pdf")]
    
    if not pdf_files:
        print("‚ùå Keine PDF-Dateien im aktuellen Ordner gefunden.")
        return []
    
    print(f"üìö {len(pdf_files)} PDF-Dateien gefunden:")
    for pdf in pdf_files:
        print(f"  - {pdf}")
    
    all_documents = []
    
    for pdf_file in pdf_files:
        print(f"\nüìñ Lade: {pdf_file}")
        documents = load_document(pdf_file)
        all_documents.extend(documents)
    
    print(f"\n‚úÖ Insgesamt {len(all_documents)} Seiten aus {len(pdf_files)} PDFs geladen")
    return all_documents


def split_text(document_list):    
    '''
    Diese Funktion unterteilt die Dokumente in kleinere Textabschnitte (Chunks).
    Beh√§lt sowohl Dokumentname als auch Seitenzahl in den Metadaten.
    '''
    text_splitter = RecursiveCharacterTextSplitter(             # wir erstellen eine Instanz des TextSplitters
        chunk_size=500,                                         # ein Dokument wird in Chunks von chunk_size Zeichen unterteilt
        chunk_overlap=150,                                       # Zwischen aufeinanderfolgenden Chunks gibt es eine √úberlappung von chunk_overlap Zeichen (f√ºr semantische √Ñhnlichkeit) 
        length_function=len,                                    # die L√§nge eines Chunks wird mit der Standard-L√§ngenfunktion gemessen
        add_start_index=True,                                   # f√ºgt den Startindex des Chunks als Metadaten hinzu
    )

    chunks = text_splitter.split_documents(document_list)       # wir teilen die Dokumente in Chunks auf, wobei jedes Dokument eine Seite repr√§sentiert
    
    print(f"‚úÇÔ∏è Das Dokument wurde in {len(chunks)} Chunks unterteilt.")     # Gibt die Anzahl der Chunks aus
    return chunks                                                          # Gibt die Liste der Chunks zur√ºck


def create_vectordb(chunks, db_path):
    '''
    Erstellt eine neue Vektordatenbank aus den Chunks.
    '''
   
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)  # Erstellt ein Embedding-Modell mit dem konfigurierten Modellnamen
    
    print(f"üõ†Ô∏è Neue Vektordatenbank wird erstellt: {db_path}")
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory=db_path
    )
    vectordb.persist()
    
    print(f"‚úÖ Vektordatenbank wurde gespeichert!")
    return vectordb

def debug_chunks(chunks, num_chunks_to_show=3):
    '''
    Debug-Funktion um zu sehen, wie die Chunks aussehen
    '''
    print(f"\n=== DEBUG: Erste {num_chunks_to_show} Chunks ===")
    for i, chunk in enumerate(chunks[:num_chunks_to_show]):
        print(f"\n--- Chunk {i+1} ---")
        print(f"L√§nge: {len(chunk.page_content)} Zeichen")
        print(f"Metadata: {chunk.metadata}")
        print(f"Inhalt: {chunk.page_content[:200]}...")


def main():
    """
    Hauptfunktion f√ºr das Preprocessing - verarbeitet alle PDFs im Ordner.
    """
    print("üîÑ Multi-PDF-Preprocessing startet...")
    
    # Datenbank-Pfad f√ºr alle PDFs
    db_path = "./chroma_dbs/all_documents"
    
    # Pr√ºfen ob Datenbank schon existiert
    if os.path.exists(db_path):
        print(f"‚ö†Ô∏è Datenbank existiert bereits: {db_path}")
        overwrite = input("M√∂chtest du sie √ºberschreiben? (j/n): ")
        if overwrite.lower() not in ["j", "ja", "y", "yes"]:
            print("‚ùå Preprocessing abgebrochen.")
            return
    
    # Alle PDFs laden
    print("üìö Alle PDFs werden geladen...")
    all_documents = load_all_pdfs_in_folder()
    
    if not all_documents:
        print("‚ùå Keine Dokumente geladen.")
        return
    
    # Text in Chunks aufteilen
    print("‚úÇÔ∏è Text wird in Chunks aufgeteilt...")
    chunks = split_text(all_documents)
    
    # Vektordatenbank erstellen
    print("üõ†Ô∏è Vektordatenbank wird erstellt...")
    vectordb = create_vectordb(chunks, db_path)
    
    print(f"üéâ Preprocessing abgeschlossen! Alle PDFs in einer Datenbank: {db_path}")


if __name__ == "__main__":
    main()