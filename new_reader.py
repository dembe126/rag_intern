# source .venv/bin/activate

# Einfache Hauptdatei - startet das komplette RAG-System mit Docling

import os
from config import DB_BASE_PATH, DB_NAME
from new_preprocess import OptimizedRAGPreprocessor  # Deine neue Docling-basierte Klasse
from new_retrieval import OptimizedRAGRetriever, setup_ollama_model  # Deine neuen Retrieval-Funktionen


def show_retrieved_chunks(chunks, show_metadata=False):
    """
    Zeigt die gefundenen Chunks zur Transparenz an - erweitert fÃ¼r Docling-Metadaten.
    """
    print(f"\nğŸ” {len(chunks)} relevante Chunks gefunden:")
    print("=" * 60)
    
    for i, chunk in enumerate(chunks, 1):
        doc_name = chunk.metadata.get('document_name', 'Unbekannt')
        
        # Seitenzahl aus Docling-Metadaten
        page_info = ""
        if 'page_numbers' in chunk.metadata and chunk.metadata['page_numbers']:
            pages = chunk.metadata['page_numbers']
            if isinstance(pages, list):
                if len(pages) == 1:
                    page_info = f" (Seite {pages[0]})"
                else:
                    page_info = f" (Seiten {'-'.join(map(str, sorted(pages)))})"
            else:
                page_info = f" (Seite {pages})"
        elif 'page_number' in chunk.metadata:
            page_info = f" (Seite {chunk.metadata['page_number']})"
        
        # Ãœberschrift aus Docling-Metadaten
        heading_info = ""
        if 'headings' in chunk.metadata and chunk.metadata['headings']:
            headings = chunk.metadata['headings']
            if isinstance(headings, list) and headings:
                heading_info = f" | {headings[0]}"
            elif isinstance(headings, str):
                heading_info = f" | {headings}"
        
        # Content preview
        content_preview = chunk.page_content[:150] + "..." if len(chunk.page_content) > 150 else chunk.page_content
        
        print(f"\nğŸ“„ CHUNK {i} - {doc_name}{page_info}{heading_info}")
        print(f"ğŸ“ LÃ¤nge: {len(chunk.page_content)} Zeichen")
        print(f"ğŸ“ {content_preview}")
        
        if show_metadata and chunk.metadata:
            print(f"ğŸ·ï¸ Metadaten: {chunk.metadata}")
        
        print("-" * 40)


def show_system_status(db_path, vectordb, model_name):
    """
    Zeigt den aktuellen System-Status an
    """
    print("\nğŸ“Š SYSTEM-STATUS")
    print("=" * 50)
    print(f"ğŸ—ƒï¸ Datenbank: {os.path.basename(db_path)}")
    print(f"ğŸ¤– LLM-Modell: {model_name}")
    print(f"ğŸ” Embedding: {os.path.basename(os.getenv('EMBEDDING_MODEL_NAME', 'Standard'))}")
    
    if vectordb:
        try:
            collection_count = vectordb._collection.count()
            print(f"ğŸ“š Chunks in DB: {collection_count}")
        except:
            print(f"ğŸ“š Chunks in DB: VerfÃ¼gbar")
    
    print("=" * 50)


def interactive_qa_loop(retriever, vectordb, model_name, db_path):
    """
    Hauptschleife fÃ¼r interaktive Fragen mit erweiterten Features
    """
    print(f"\nğŸ’¬ Interaktives Q&A gestartet!")
    print("ğŸ’¡ VerfÃ¼gbare Kommandos:")
    print("  - 'status' : System-Informationen anzeigen")
    print("  - 'debug'  : NÃ¤chste Antwort mit Chunk-Details")
    print("  - 'help'   : Diese Hilfe anzeigen")
    print("  - 'exit'   : Programm beenden")
    print("-" * 60)
    
    debug_mode = False
    
    while True:
        question = input("\nâ“ Deine Frage: ").strip()
        
        # Kommandos verarbeiten
        if question.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ RAG-System beendet!")
            break
        
        elif question.lower() == "help":
            print("\nğŸ“– HILFE:")
            print("  - Stelle Fragen zu den geladenen PDF-Dokumenten")
            print("  - 'status' zeigt System-Informationen")
            print("  - 'debug' aktiviert Details fÃ¼r die nÃ¤chste Frage")
            print("  - 'exit' beendet das Programm")
            continue
        
        elif question.lower() == "status":
            show_system_status(db_path, vectordb, model_name)
            continue
        
        elif question.lower() == "debug":
            debug_mode = True
            print("ğŸ” Debug-Modus fÃ¼r nÃ¤chste Frage aktiviert!")
            continue
        
        if not question:
            continue
        
        print("\nğŸ”„ Verarbeitung lÃ¤uft...")
        
        # Chunks abrufen und anzeigen (deine bestehende Logik)
        retriever_engine = vectordb.as_retriever(search_kwargs={'k': 8})
        retrieved_chunks = retriever_engine.invoke(question)
        
        if debug_mode:
            show_retrieved_chunks(retrieved_chunks, show_metadata=True)
            debug_mode = False
        else:
            show_retrieved_chunks(retrieved_chunks, show_metadata=False)
        
        # Erweiterte Abfrage mit dem neuen Retriever
        response = retriever.query_with_enhanced_response(question)
        
        if "error" in response:
            print(f"âŒ {response['error']}")
            continue
        
        # Antwort anzeigen
        print(f"\nğŸ’¡ ANTWORT:")
        print("-" * 50)
        print(response['answer'])
        
        # Verbesserte Quellen anzeigen
        if response['sources']:
            stats = response['chunk_stats']
            print(f"\nğŸ“š QUELLEN ({stats['total_chunks']} Chunks aus {stats['unique_documents']} Dokument(en)):")
            for source in response['sources']:
                print(f"  {source}")
        
        print("\n" + "="*60)


def main():
    """
    Startet das komplette optimierte RAG-System.
    """
    
    print("ğŸš€ Optimiertes RAG-System mit Docling wird gestartet...")
    
    # 1. Datenbank-Pfad definieren
    db_path = os.path.join(DB_BASE_PATH, "optimized_rag_docling")
    
    # 2. Preprocessor fÃ¼r Docling-basierte Verarbeitung
    preprocessor = OptimizedRAGPreprocessor(max_tokens=400, overlap_tokens=50)
    
    # 3. Datenbank laden oder erstellen
    if os.path.exists(db_path):
        print(f"\nğŸ“‚ Schritt 1: Bestehende Docling-Datenbank wird geladen...")
        
        # Retriever initialisieren und Datenbank laden
        retriever = OptimizedRAGRetriever()
        vectordb = retriever.load_vectordb(db_path)
        
        if vectordb is None:
            print("âŒ Fehler beim Laden der Datenbank.")
            return
            
    else:
        print(f"\nğŸ› ï¸ Schritt 1: Neue Docling-Datenbank wird erstellt...")
        print("ğŸ“‹ Alle PDFs im aktuellen Ordner werden verarbeitet...")
        
        # Neue Datenbank mit allen PDFs erstellen
        vectordb = preprocessor.process_all_pdfs(db_name="optimized_rag_docling")
        
        if vectordb is None:
            print("âŒ Fehler beim Erstellen der Datenbank.")
            return
        
        # Retriever nach der Erstellung initialisieren
        retriever = OptimizedRAGRetriever()
        retriever.vectordb = vectordb
    
    # 4. Ollama-Modell einrichten (deine bestehende Funktion)
    print(f"\nğŸ¤– Schritt 2: Ollama-Modell wird eingerichtet...")
    model = setup_ollama_model()  # Nutzt deine bewÃ¤hrte Modell-Auswahl
    if model is None:
        print("âŒ Kein Modell verfÃ¼gbar. Programm beendet.")
        return
    
    # 5. RAG-Chain mit dem neuen Retriever aufbauen
    print(f"\nâš™ï¸ Schritt 3: Optimierte RAG-Chain wird aufgebaut...")
    qa_chain = retriever.setup_rag_chain(model, retrieval_k=6)
    if qa_chain is None:
        print("âŒ Fehler beim Aufbau der RAG-Chain.")
        return
    
    # 6. System-Status anzeigen
    show_system_status(db_path, vectordb, model)
    
    # 7. Interaktive Q&A-Schleife (erweiterte Version)
    interactive_qa_loop(retriever, vectordb, model, db_path)


if __name__ == "__main__":
    main()